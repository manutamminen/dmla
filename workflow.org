* 1) Figure 2

** Join the paired ends and quality filter using nsearch

|-----------------------------------------+---------------|
| Inputs                                  | Outputs       |
|-----------------------------------------+---------------|
| NG-17872_10_lib297291_6185_1_1.fastq.gz | lib10.fasta   |
| NG-17872_11_lib297292_6178_3_1.fastq.gz | lib11_1.fasta |
| NG-17872_11_lib297292_6189_3_1.fastq.gz | lib11_2.fasta |
| NG-17872_10_lib297291_6185_1_1.fastq.gz |               |
| NG-17872_11_lib297292_6178_3_1.fastq.gz |               |
| NG-17872_11_lib297292_6189_3_1.fastq.gz |               |
|-----------------------------------------+---------------|

 #+BEGIN_SRC sh
 nsearch merge --forward NG-17872_10_lib297291_6185_1_1.fastq.gz --reverse NG-17872_10_lib297291_6185_1_2.fastq.gz --out lib10.fastq
 nsearch merge --forward NG-17872_11_lib297292_6178_3_1.fastq.gz --reverse NG-17872_11_lib297292_6178_3_2.fastq.gz --out lib11_1.fastq
 nsearch merge --forward NG-17872_11_lib297292_6189_3_1.fastq.gz --reverse NG-17872_11_lib297292_6189_3_2.fastq.gz --out lib11_2.fastq

 nsearch filter --in lib10.fastq --out lib10.fasta
 nsearch filter --in lib11_1.fastq --out lib11_1.fasta
 nsearch filter --in lib11_2.fastq --out lib11_2.fasta
 #+END_SRC


** Then process the merged, quality-filtered sequences into count tables on Python

|-------------+-------------|
| Inputs      | Outputs     |
|-------------+-------------|
| probes.xlsx | lib10.csv   |
|             | lib11_1.csv |
|             | lib11_2.csv |
|-------------+-------------|

 #+BEGIN_SRC python
 import os
 import epride as ep
 import pandas as pd
 from collections import defaultdict

 ## Import the data

 probes = pd.ExcelFile("probes.xlsx").parse('probes')
 pcr_bcs = pd.ExcelFile("probes.xlsx").parse('pcr_barcodes').drop('Sequence', axis=1)
 other_sequences = pd.ExcelFile("probes.xlsx") \
                     .parse('other_primers_and_sequences') \
                     .set_index('Sequence_name')
 left_side = other_sequences.loc['for_primer_5', 'Sequence']
 middle = other_sequences.loc['left_probe_5', 'Sequence']
 right_side = other_sequences.loc['rev_primer_rc', 'Sequence'][:20]


 ## Create the template, sample id and bc number dictionaries

 template_dictionary = {}
 for _, row in probes.iterrows():
     for seq in ep.expand_primers(row['Target']):
         template_dictionary[seq] = row['Short_name']

 sample_id_dict = {bc: bc_id for _, (_, bc_id, bc) in pcr_bcs.iterrows()}

 sample_ix_dict = {bc: ix for _, (ix, _, bc) in pcr_bcs.iterrows()}


 ## Define the sequence parser

 def seq_parser(fasta_file):
     for seq_id, seq in ep.read_fasta(fasta_file):
         if (len(seq) > 133 or len(seq) < 140) and \
         seq.count(left_side) == 1 and \
         seq.count(middle) == 1 and \
         seq.count(right_side) == 1:
             cluster_id = ''
             try:
                 fst_half, long_mid_part = seq.split(middle)
                 _, bc = fst_half.split(left_side)
                 mid_part, _ = long_mid_part.split(right_side)
                 mol_id = mid_part[-10:]
                 cluster_id = mid_part[8:-10]
                 if bc in sample_id_dict:
                     sample_id = sample_id_dict[bc]
                     sample_ix = sample_ix_dict[bc]
             except ValueError:
                 pass
             if cluster_id in template_dictionary:
                 cluster = template_dictionary[cluster_id]
                 yield [sample_ix, sample_id, cluster, mol_id]

 ## And parse the sequences into pandas DataFrames

 lib10 = pd.DataFrame(seq_parser("lib10.fasta"),
                      columns=['Sample_ix',
                               'Sample_id',
                               'Cluster',
                               'Molecule_id'])

 lib11_1 = pd.DataFrame(seq_parser("lib11_1.fasta"),
                        columns=['Sample_ix',
                                 'Sample_id',
                                 'Cluster',
                                 'Molecule_id'])

 lib11_2 = pd.DataFrame(seq_parser("lib11_2.fasta"),
                        columns=['Sample_ix',
                                 'Sample_id',
                                 'Cluster',
                                 'Molecule_id'])

 ## And write out as csvs

 lib10.to_csv("lib10.csv", index=False)
 lib11_1.to_csv("lib11_1.csv", index=False)
 lib11_2.to_csv("lib11_2.csv", index=False)

 #+END_SRC

 
** Expand the library file (which lists the gene families present in the bacterial genomic DNA samples)

|----------------+-------------------------|
| Inputs         | Outputs                 |
|----------------+-------------------------|
| libraries.xlsx | expanded_libraries.xlsx |
|----------------+-------------------------|

#+BEGIN_SRC ipython :session
import os
import epride as ep
import pandas as pd
from collections import defaultdict

## Import the data

libraries = pd.read_excel("libraries.xlsx")

## Expand the table based in the numeric Cluster column

acc = []
for _, row in libraries.iterrows():
    cluster = row['Cluster']
    if isinstance(cluster, int):
        row1 = row.copy().to_dict()
        row2 = row.copy().to_dict()
        row1['Cluster'] = str(cluster) + "_1"
        row2['Cluster'] = str(cluster) + "_2"
        acc.append(row1)
        acc.append(row2)
    elif "," in cluster:
        exp_cluster = cluster.split(",")
        for cluster_instance in exp_cluster:
            try:
                cluster_instance = int(cluster_instance)
                row1 = row.copy().to_dict()
                row2 = row.copy().to_dict()
                row1['Cluster'] = str(cluster_instance) + "_1"
                row2['Cluster'] = str(cluster_instance) + "_2"
                acc.append(row1)
                acc.append(row2)
            except ValueError:
                pass

exp_libraries = pd.DataFrame(acc)[['Number',
                                   'Sample_ID',
                                   'Genes',
                                   'Cluster',
                                   'Probes_in_MM_included',
                                   'Sample_ix',
                                   'Tube']]

exp_libraries.to_excel("expanded_libraries.xlsx", index=False)
#+END_SRC


** Prepare visualizations of the lib10 and lib11 count tables

|-------------------------+------------------|
| Inputs                  | Outputs          |
|-------------------------+------------------|
| expanded_libraries.xlsx | lib_complete.pdf |
| lib10.csv               |                  |
| lib11_1.csv             |                  |
| lib11_2.csv             |                  |
|-------------------------+------------------|

 #+BEGIN_SRC R :session
 library(tidyverse)
 library(readxl)

 ## Prepare count table for tube 10

 lib10_counts <-
     read_csv("lib10.csv") %>%
     unique %>%
     group_by(Sample_ix, Cluster) %>%
     summarise(n=n()) %>%
     spread(key=Cluster, value=n, fill=0) %>%
     ungroup %>%
     mutate(Sample_ix = as.factor(Sample_ix)) %>%
     gather(Cluster, Count, -Sample_ix) %>%
     mutate(Tube = 10) %>%
     select(Tube, Sample_ix, Cluster, Count)

 ## Prepare count table for tube 11

 lib11_counts <-
     rbind(
         read_csv("lib11_1.csv"),
         read_csv("lib11_2.csv")) %>%
     unique %>%
     group_by(Sample_ix, Cluster) %>%
     summarise(n=n()) %>%
     spread(key=Cluster, value=n, fill=0) %>%
     ungroup %>%
     mutate(Sample_ix = as.factor(Sample_ix)) %>%
     gather(Cluster, Count, -Sample_ix) %>%
     mutate(Tube = 11) %>%
     select(Tube, Sample_ix, Cluster, Count)

 ## Merge the count tables

 lib_counts <-
     rbind(lib10_counts, lib11_counts) %>%
     spread(Cluster, Count, fill=0) %>%
     gather(Cluster, Count, -Tube, -Sample_ix)  %>%
     spread(Sample_ix, Count, fill=0) %>%
     gather(Sample_ix, Count, -Tube, -Cluster)
 

 ## Prepare a logical mask of the sample design

 design <-
     read_excel("expanded_libraries.xlsx") %>%
     mutate(Entry = 1) %>%
     select(Tube, Cluster, Tube, Sample_ix, Entry) %>%
     unique %>%
     spread(Cluster, Entry, fill=0) %>%
     mutate(Sample_ix = as.factor(Sample_ix)) %>%
     gather(Cluster, Count, -Sample_ix, -Tube) %>%
     mutate(Mask = Count > 0) %>%
     select(-Count)

 ## Merge the logical mask with the count tables

 full_lib <-
     left_join(lib_counts,
               design,
               by=c("Tube", "Sample_ix", "Cluster")) %>%
     mutate_if(is.logical, replace_na, FALSE) %>%
     mutate(Cluster = as.factor(Cluster))

 ## Plot as a heatmap and reverse the false positives for visual identification

 full_lib %>% 
     mutate(Count = ifelse(Mask, Count, -Count),
            Tube = as.factor(Tube)) %>%
     ggplot(aes(x=Cluster, y=Sample_ix)) +
     geom_tile(aes(fill=Count), color="gray") +
     facet_grid(Tube~.) +
     scale_fill_gradient2(low = "blue", high = "red", mid="white") +
     theme(axis.text.x = element_text(angle=45, hjust=1, size=7),
           axis.text.y = element_text(size=5))
 ggsave("lib_complete.pdf", last_plot())
 
 ## Summarise the clusters per sample per tube

 cluster_summary <-
     filter(full_lib, Count > 500) %>%
     separate(Cluster, into=c("Cluster_no", "Cluster_repl"), sep="_") %>%
     select(-Cluster_repl, -Mask, -Count) %>%
     group_by(Tube, Sample_ix) %>%
     summarise(Clusters = paste(unique(Cluster_no), collapse=","))
 write_delim(cluster_summary, "cluster_summary.csv", delim=";")


 t10_dl <- 
     filter(full_lib, Tube == 10,
            Sample_ix %in% c(511, 512, 513)) %>%
     group_by(Cluster) %>% 
     summarise(mean_cnt = mean(Count, na.rm = TRUE),
               sd_cnt = sd(Count, na.rm = TRUE),
               ld = mean_cnt + 3*sd_cnt)

 full_lib %>% 
     filter(Tube == 10) %>% 
     filter(!(Sample_ix %in% c(511, 512, 513))) %>%
     left_join(t10_dl, by="Cluster") %>%
     ungroup %>% 
     mutate(Signal = ifelse(Count > ld, 1, 0),
            Sample_ix = as.factor(Sample_ix),
            Signal = ifelse(Mask, Signal, -Signal)) %>% 
     ggplot(aes(x=Cluster, y=Sample_ix)) +
     geom_tile(aes(fill=Signal)) +
     scale_fill_gradient2(low = "blue", mid = "white", high = "red") +
     theme(axis.text.x = element_text(angle=45, hjust=1, size=7),
           axis.text.y = element_text(size=7))

 ggsave("interm1.pdf", last_plot())

 full_lib %>% 
     filter(Tube == 10) %>% 
     filter(!(Sample_ix %in% c(511, 512, 513))) %>%
     left_join(t10_dl, by="Cluster") %>%
     ungroup %>% 
     mutate(Signal = ifelse(Count > ld, 1, 0),
            Sample_ix = as.factor(Sample_ix)) %>% 
     separate(Cluster, c("Clust", "Repl"), by="_") %>%
     group_by(Sample_ix, Clust) %>%
     mutate(Signal = ifelse(sum(Signal) == 2, 1, 0),
            Signal = ifelse(Mask, Signal, -Signal)) %>% 
     ggplot(aes(x=Clust, y=Sample_ix)) +
     geom_tile(aes(fill=Signal)) +
     scale_fill_gradient2(low = "blue", mid = "white", high = "red") +
     theme(axis.text.x = element_text(angle=45, hjust=1, size=7),
           axis.text.y = element_text(size=7))

 ggsave("final1.pdf", last_plot())



 clust_conv <- 
     read_xlsx("Table_Gates_ProbeTargets.xlsx") %>%
     select(Cluster, `Enzyme family`) %>%
     filter(complete.cases(.)) %>%
     rename(Clust = Cluster,
            Family = `Enzyme family`)


 full_lib %>% 
     filter(Tube == 10) %>% 
     filter(!(Sample_ix %in% c(511, 512, 513))) %>%
     left_join(t10_dl, by="Cluster") %>%
     ungroup %>% 
     mutate(Signal = ifelse(Count > ld, 1, 0),
            Sample_ix = as.numeric(Sample_ix)) %>% 
     separate(Cluster, c("Clust", "Repl"), by="_") %>%
     group_by(Sample_ix, Clust) %>%
     mutate(Signal = ifelse(sum(Signal) == 2, 1, 0),
            Signal = ifelse(Mask, Signal, -Signal)) %>%
     ungroup %>%
     mutate(Clust = as.numeric(Clust)) %>% 
     left_join(read_xlsx("sample_conversion.xlsx")) %>%
     left_join(clust_conv) %>% 
     ggplot(aes(x=Family, y=Sample_ID)) +
     geom_tile(aes(fill=Signal)) +
     scale_fill_gradient2(low = "blue", mid = "white", high = "red") +
     theme(axis.text.x = element_text(angle=45, hjust=1, size=7),
           axis.text.y = element_text(size=7))

 ggsave("final1.pdf", last_plot())

 full_lib %>% 
     filter(Tube == 10) %>% 
     filter(!(Sample_ix %in% c(511, 512, 513))) %>%
     left_join(t10_dl, by="Cluster") %>%
     ungroup %>% 
     mutate(Signal = ifelse(Count > ld, 1, 0),
            Sample_ix = as.numeric(Sample_ix)) %>% 
     separate(Cluster, c("Clust", "Repl"), by="_") %>%
     group_by(Sample_ix, Clust) %>%
     mutate(Signal = ifelse(sum(Signal) == 2, 1, 0)) %>%
     ungroup %>%
     mutate(Clust = as.numeric(Clust)) %>% 
     left_join(read_xlsx("sample_conversion.xlsx")) %>%
     left_join(clust_conv) %>%
     select(-Tube, -Repl, -Count, -Mask, -mean_cnt, -sd_cnt, -ld) %>%
     filter(Signal == 1) %>%

     write_csv("double_positives.csv")

 full_lib %>% 
     filter(Tube == 10) %>% 
     filter(!(Sample_ix %in% c(511, 512, 513))) %>%
     left_join(t10_dl, by="Cluster") %>%
     ungroup %>% 
     mutate(Signal = ifelse(Count > ld, 1, 0),
            Sample_ix = as.numeric(Sample_ix)) %>% 
     separate(Cluster, c("Clust", "Repl"), by="_") %>%
     group_by(Sample_ix, Clust) %>%
     mutate(Signal = ifelse(sum(Signal) == 2, 1, 0),
            Signal = ifelse(Mask, Signal, -Signal)) %>% 
     ungroup %>% 
     mutate(Clust = as.factor(as.numeric(Clust)),
            Sample_ix = as.factor(Sample_ix)) %>% 
     ggplot(aes(x=Clust, y=Sample_ix)) +
     geom_tile(aes(fill=Signal)) +
     scale_fill_gradient2(low = "blue", mid = "white", high = "red") +
     theme(axis.text.x = element_text(angle=45, hjust=1, size=7),
           axis.text.y = element_text(size=7))
 ggsave("final2.pdf", last_plot())



 conf_mask <-
     read_xlsx("cluster_confirmation4.xlsx") %>% 
     gather(Cf, Conf, -Sample_ix, -False1, -False2, -False3) %>%
     select(-Cf) %>%
     gather(Fl, False_pos, -Sample_ix, -Conf) %>%
     select(-Fl) %>%
     filter(!(is.na(Conf) & is.na(False_pos))) %>%
     unique %>%
     mutate(Category = case_when(
                !is.na(Conf) ~ 3,
                !is.na(False_pos) ~ 4)) %>%
     gather(Type, Clust, -Sample_ix, -Category) %>%
     filter(complete.cases(.)) %>%
     select(-Type)


 signal_tbl <- 
     full_lib %>% 
     filter(Tube == 10) %>% 
     filter(!(Sample_ix %in% c(511, 512, 513))) %>%
     left_join(t10_dl, by="Cluster") %>%
     ungroup %>% 
     mutate(Signal = ifelse(Count > ld, 1, 0),
            Sample_ix = as.numeric(Sample_ix)) %>% 
     separate(Cluster, c("Clust", "Repl"), by="_") %>%
     group_by(Sample_ix, Clust) %>%
     mutate(Signal = ifelse(sum(Signal) == 2, 1, 0),
            Signal = ifelse(Mask, Signal, -Signal)) %>%
     ungroup %>%
     mutate(Clust = as.numeric(Clust)) %>%
     full_join(conf_mask) %>%
     left_join(read_xlsx("sample_conversion.xlsx")) %>%
     left_join(clust_conv) %>%
     select(-mean_cnt, -sd_cnt, -ld) %>% 
     mutate(Sign = case_when(
                Signal == -1 ~ 2,
                is.na(Category) ~ Signal,
                !is.na(Category) ~ Category),
            Sign = ifelse(Signal == 0, 0, Sign),
            Sign = as.factor(Sign))


 ggplot(signal_tbl, aes(x=Family, y=Sample_ID)) +
     geom_tile(aes(fill=Sign)) +
     scale_fill_manual(values = c("white", "red", "blue", "green", "orange")) +
     theme(axis.text.x = element_text(angle=45, hjust=1, size=7),
           axis.text.y = element_text(size=7))
 ggsave("final5.pdf", last_plot())
 
 ggplot(signal_tbl, aes(x=Family, y=Sample_ID)) +
     geom_tile(aes(fill=Sign)) +
     scale_fill_manual(values = c("white", "red", "blue", "green", "orange")) +
     theme(axis.text.x = element_text(angle=45, hjust=1, size=7),
           axis.text.y = element_text(size=7))

 ggsave("final4.pdf", last_plot())

 signal_tbl %>%
     mutate(Clust = as.factor(Clust),
            Sample_ix = as.factor(Sample_ix)) %>% 
     ggplot(aes(x=Clust, y=Sample_ix)) +
     geom_tile(aes(fill=Sign)) +
     scale_fill_manual(values = c("white", "red", "blue", "green", "orange")) +
     theme(axis.text.x = element_text(angle=45, hjust=1, size=7),
           axis.text.y = element_text(size=7))

 ggsave("overlaid.pdf", last_plot())

 signal_tbl %>%
     mutate(Clust = as.factor(Clust),
            Sample_ix = as.factor(Sample_ix),
            Signal = as.factor(Signal)) %>% 
     ggplot(aes(x=Clust, y=Sample_ix)) +
     geom_tile(aes(fill=Signal)) +
     scale_fill_manual(values = c("blue", "white", "red")) +
     theme(axis.text.x = element_text(angle=45, hjust=1, size=7),
           axis.text.y = element_text(size=7))
 ggsave("non_overlaid.pdf", last_plot())

 full_lib %>% 
     mutate(Count = ifelse(Mask, Count, -Count),
            Tube = as.factor(Tube)) %>% 
     filter(Tube == 10) %>% 
     separate(Cluster, c("Clust", "Repl"), by="_") %>% 
     mutate(Sample_ix = as.numeric(Sample_ix),
            Clust = as.numeric(Clust),
            Count = abs(Count)) %>%
     left_join(read_xlsx("sample_conversion.xlsx")) %>%
     left_join(clust_conv) %>% 
     filter(Repl != 3,
            Family != "KPC") %>% 
     unite(Fam_rep, Family, Repl, sep=" replicate") %>% 
     ggplot(aes(x=Fam_rep, y=Sample_ID)) +
     geom_tile(aes(fill=Count), color="gray") +
     scale_fill_gradient2(low = "blue", high = "red", mid="white") +
     theme(axis.text.x = element_text(angle=45, hjust=1, size=7),
           axis.text.y = element_text(size=5))
 ggsave("raw.pdf", last_plot())

 
 lib1 <- 
     full_lib %>% 
     mutate(Count = ifelse(Mask, Count, -Count),
            Tube = as.factor(Tube)) %>% 
     filter(Tube == 10) %>% 
     separate(Cluster, c("Clust", "Repl"), by="_") %>% 
     mutate(Sample_ix = as.numeric(Sample_ix),
            Clust = as.numeric(Clust),
            Count = abs(Count)) %>%
     left_join(read_xlsx("sample_conversion.xlsx")) %>%
     left_join(clust_conv) %>% 
     filter(Repl != 3,
            Family != "KPC") %>% 
     select(Family, Repl, Sample_ID, Count)

 lib2 <-
     signal_tbl %>%
     select(Family, Repl, Sample_ID, Sign)

 left_join(lib1, lib2) %>% 
     unite(Fam_rep, Family, Repl, sep=" replicate") %>% 
     ggplot(aes(x=Fam_rep, y=Sample_ID)) +
     geom_tile(aes(fill=Count, color=Sign), size=1) +
     scale_fill_gradient2(low = "blue", high = "red", mid="white") +
     theme(axis.text.x = element_text(angle=45, hjust=1, size=7),
           axis.text.y = element_text(size=5))
 ggsave("raw_joined.pdf", last_plot())

 
 left_join(lib1, lib2) %>% 
     unite(Fam_rep, Family, Repl, sep=" replicate") %>%
     filter(complete.cases(.),
            Sign != 0) %>%
     pull(Sample_ID)

 left_join(lib1, lib2) %>% 
     unite(Fam_rep, Family, Repl, sep=" replicate") %>%
     pull(Sample_ID) %>%
     unique %>%
     length
 

 #+END_SRC


* 2) Figure S01 

** Prepare the 40-mer probe candidates
   
|---------------+-------------------|
| Inputs        | Outputs           |
|---------------+-------------------|
| Allele-dna.fa | probe_cands.fasta |
|---------------+-------------------|

 #+BEGIN_SRC R
 library(tidyverse)
 library(readxl)
 library(Biostrings)
 library(igraph)
 library(DECIPHER)
 library(stringi)
 library(glue)
 library(readxl)

 sliding_window <- function(sequence, win_size=20)
 {
     win_size <- win_size - 1
     split_sequence <- strsplit(sequence, split="")[[1]]
     num_chunks <- length(split_sequence) - win_size
     acc <- vector(mode = "character",
                   length = num_chunks)
     for (i in 1:num_chunks)
     {
         sub_seq <- paste(split_sequence[i : (i + win_size)],
                          collapse = "")
         acc[i] <- sub_seq
     }
     acc
 }

 deg_list <-
     list(
         'A' = 'A',
         'T' = 'T',
         'G' = 'G',
         'C' = 'C',
         '-' = '-',
         'W' = c('A', 'T'),
         'S' = c('C', 'G'),
         'M' = c('A', 'C'),
         'K' = c('G', 'T'),
         'R' = c('A', 'G'),
         'Y' = c('C', 'T'),
         'B' = c('C', 'G', 'T'),
         'D' = c('A', 'G', 'T'),
         'H' = c('A', 'C', 'G'),
         'V' = c('A', 'C', 'T'),
         'N' = c('A', 'C', 'G', 'T'))

 expand_seq <- function(seq)
 {
     seq_lst <-
         strsplit(seq, "") %>%
         unlist %>%
         map(~deg_list[[.x]]) %>%
         purrr::reduce(~as.vector(outer(.x, .y, paste, sep="")))
     if (identical(seq_lst, character(0)))
     {
         stop("Not a DNA sequence!")
     } else {
         seq_lst
     }
 }

 fasta_to_df <- function(filename)
 {
     fasta <- readDNAStringSet(filename)
     seqs <- as.character(fasta)
     names(seqs) <- NA
     tibble(Name = names(fasta),
            Sequence = seqs)
 }
 
 primer_candidates <-
     fasta_to_df("Allele-dna.fa") %>% 
     mutate(Exp = map(Sequence,
                      sliding_window(as.character,
                                     win_size = 40))) %>%
     select(-Sequence) %>%
     unnest(Exp) %>%
     group_by(Name) %>%
     mutate(Ix = row_number()) %>%
     separate(Name, into=c("Prot_id"), sep=" ") %>%
     unite(Fasta_id, Prot_id, Ix, sep="_")

 primer_candidates %>% 
     mutate(Out = glue(">{Fasta_id}\n{Exp}\n")) %>% 
     pull(Out) %>%
     write("probe_cands.fasta")
 #+END_SRC


** Prepare the BLAST search table

|-------------------+----------------|
| Inputs            | Outputs        |
|-------------------+----------------|
| probe_cands.fasta | probe_hits.csv |
| Allele-dna.fa     |                |
|-------------------+----------------|

 #+BEGIN_SRC sh 
 nsearch search --query=probe_cands.fasta --db=Allele-dna.fa --out=probe_hits.csv --min-identity=0.8 --strand=both --max-hits=1558
 #+END_SRC


** Then parse the resulting output file "probe_hits.csv" using a memory-efficient Python script

|----------------+------------------|
| Inputs         | Outputs          |
|----------------+------------------|
| probe_hits.csv | probe_counts.csv |
|----------------+------------------|

 #+BEGIN_SRC python
 import sys
 from collections import defaultdict

 acc = defaultdict(int)
 with open('probe_hits.csv') as fh:
     next(fh)
     for ix, ln in enumerate(fh):
         broken = ln.split(",")
         fst = broken[0].replace("WP_", "WP").split("_")[0]
         fst = fst.replace("WP", "WP_")
         snd = broken[1].replace("WP_", "WP").split(" ")[0]
         snd = snd.replace("WP", "WP_")
         qlength = int(broken[3]) - int(broken[2])
         tlength = int(broken[5]) - int(broken[4])
         to_acc = ",".join(sorted([fst, snd]))
         if ((qlength == tlength) and (qlength == 39) and (fst != snd)):
             acc[to_acc] += 1
         if (ix % 100000 == 0):
             print(ix)

 with open('probe_counts.csv', 'w') as fh:
     for key, val in acc.items():
         fh.write(key + "\n")
 #+END_SRC


** Prepare the gdf file from probe_counts.csv

|------------------+--------------|
| Inputs           | Outputs      |
|------------------+--------------|
| probe_counts.csv | clusters.gdf |
|------------------+--------------|

 #+BEGIN_SRC R :session
 
 con2 <- read_csv("probe_counts.csv", col_names=FALSE) %>%
     unite(Netw, X1, X2, sep=",") %>%
     pull(Netw)

 annotation <- read.delim("Allele.tab", sep="\t") %>%
     separate(allele_name, into=c("type"), sep="-", remove=FALSE) %>%
     mutate(size = stop - start) %>%
     select(protein_accession, type, size) %>%
     with(paste(protein_accession, type, size, sep=","))

 gdf <- c("nodedef>name VARCHAR,type VARCHAR,size DOUBLE",
          annotation,
          "edgedef>node1 VARCHAR,node2 VARCHAR",
          con2)
         
 write(gdf, "clusters.gdf")

 #+END_SRC


** Prepare also the gdf such that our primer designs are also shown in the network

*** Start by expanding our probe designs (all_probes.xlsx) into non-degenerate versions

|-----------------+----------------|
| Inputs          | Outputs        |
|-----------------+----------------|
| all_probes.xlsx | exp_probes.csv |
|-----------------+----------------|

 #+BEGIN_SRC R :session

 library(readxl)

 all_probes <-
     read_excel("all_probes.xlsx", sheet = "probes")

 exp_probes <- 
     all_probes %>%
     mutate(Exp = map(Target, expand_seq)) %>%
     unnest

 write_csv(exp_probes, "exp_probes.csv")

 #+END_SRC


*** Then filter out their target ranges using a memory-efficient Python script

|----------------+-------------------------|
| Inputs         | Outputs                 |
|----------------+-------------------------|
| exp_probes.csv | selected_probe_hits.csv |
| probe_hits.csv |                         |
|----------------+-------------------------|

 #+BEGIN_SRC python
 seq_acc = set()
 with open("exp_probes.csv") as ep:
     next(ep)
     for ix, line in enumerate(ep):
         seq = line.split(",")[4].strip()
         seq_acc.add(seq)

 probe_acc = []
 with open("probe_hits.csv") as ph:
     next(ph)
     for ix, line in enumerate(ph):
         seq = line.split(",")[6]
         if seq in seq_set:
             probe_acc.append(line)
         if (ix % 1000 == 0):
             print(ix)
        
 with open("selected_probe_hits.csv", "w") as out:
     for line in probe_acc:
         out.write(line)
 #+END_SRC


*** Process the resulting selected probe hits file "selected_probe_hits.csv" into gdf annotation

|-------------------------+--------------|
| Inputs                  | Outputs      |
|-------------------------+--------------|
| selected_probe_hits.csv | clusters.gdf |
| probe_counts.csv        |              |
| Allele.tab              |              |
|-------------------------+--------------|

 #+BEGIN_SRC R :session

 library(tidyverse)
 library(stringr)
 library(readxl)
 
 selected_hits <-
     read_csv("selected_probe_hits.csv", col_names=FALSE)

 exp_probes <-
     read_csv("exp_probes.csv")

 probe_coverage <-
     left_join(exp_probes, selected_hits, by=c("Exp" = "X7")) %>%
     select(Name, X1) %>%
     filter(complete.cases(.)) %>%
     unique %>%
     mutate(X1 = str_replace(X1, "WP_", "WP")) %>%
     separate(X1, c("Seq"), "_") %>%
     mutate(Seq = str_replace(Seq, "WP", "WP_")) %>%
     unique %>%
     group_by(Seq) %>%
     summarise(Probes = paste(sort(Name), collapse=";"))

 con2 <- read_csv("probe_counts.csv", col_names=FALSE) %>%
     unite(Netw, X1, X2, sep=",") %>%
     pull(Netw)

 annotation <-
     read.delim("Allele.tab", sep="\t") %>%
     separate(allele_name, into=c("type"), sep="-", remove=FALSE) %>%
     mutate(size = stop - start) %>%
     select(protein_accession, type, size) %>%
     left_join(probe_coverage, by=c("protein_accession" = "Seq"))

 annotation %>%
     filter(complete.cases(.),
            type != "NDM") %>%
     group_by(Probes) %>%
     summarise(n=n()) %>%
     arrange(desc(n)) %>% 
     data.frame
    
 gdf_annotation <- 
     annotation %>%
     with(paste(protein_accession, type, size, Probes, sep=","))

 gdf <- c("nodedef>name VARCHAR,type VARCHAR,size DOUBLE,probe VARCHAR",
          gdf_annotation,
          "edgedef>node1 VARCHAR,node2 VARCHAR",
          con2)
         
 write(gdf, "clusters.gdf")

 probe_targets <-
     left_join(selected_hits, exp_probes, c("X7" = "Exp")) %>%
     separate(X2, c("protein_accession", "Junk"), " ") %>%
     select(X1, protein_accession, Name) %>%
     left_join(annotation)

 multiple_targets <- 
     probe_targets %>%
     group_by(Probes, type) %>%
     summarise(n=n()) %>%
     group_by(Probes) %>%
     summarise(n=n()) %>%
     arrange(desc(n)) %>%
     filter(n > 1, complete.cases(.)) %>%
     pull(Probes)

 mult_targeting_probes <- 
     probe_targets %>%
     filter(Probes %in% multiple_targets) %>%
     group_by(Probes, type) %>%
     summarise(n=n()) %>%
     separate(Probes, c("Fst", "Snd"), ";") %>%
     select(Fst, Snd)
 mult_targeting_probes <-
     c(mult_targeting_probes$Fst, mult_targeting_probes$Snd) %>%
     unique

 all_probes <-
     read_excel("all_probes.xlsx", sheet = "probes")

 all_probes %>% mutate(
                  A = str_count(Target, "A"),
                  T = str_count(Target, "T"),
                  G = str_count(Target, "G"),
                  C = str_count(Target, "C"),
                  Non_deg = A + T + C + G) %>%
     filter(Name %in% mult_targeting_probes)

 all_probes %>% mutate(
                  A = str_count(Target, "A"),
                  T = str_count(Target, "T"),
                  G = str_count(Target, "G"),
                  C = str_count(Target, "C"),
                  Non_deg = A + T + C + G) %>%
     ggplot(aes(x=Non_deg)) +
     geom_density() +
     geom_vline(aes(xintercept=28)) +
     geom_vline(aes(xintercept=31)) +
     geom_vline(aes(xintercept=32))
 ggsave("degen_density.pdf", last_plot())

 #+END_SRC

 
* New stuff

#+BEGIN_SRC R :session

## Generate quality reports
## ls *.fastq.gz | while read file; do echo $file; fastqc $file ; done
## multiqc .

## nsearch merge --forward=NG-13024_1_lib236478_5794_7_1.fastq.gz --reverse=NG-13024_1_lib236478_5794_7_2.fastq.gz --out=NG-13024_1.fastq
## nsearch merge --forward=NG-13024_2_lib236479_5794_7_1.fastq.gz --reverse=NG-13024_2_lib236479_5794_7_2.fastq.gz --out=NG-13024_2.fastq
## nsearch merge --forward=NG-13024_3_lib236480_5794_7_1.fastq.gz --reverse=NG-13024_3_lib236480_5794_7_2.fastq.gz --out=NG-13024_3.fastq
## nsearch merge --forward=NG-13024_4_lib237853_5794_7_1.fastq.gz --reverse=NG-13024_4_lib237853_5794_7_2.fastq.gz --out=NG-13024_4.fastq
## nsearch merge --forward=NG-13024_5_lib237854_5794_7_1.fastq.gz --reverse=NG-13024_5_lib237854_5794_7_2.fastq.gz --out=NG-13024_5.fastq
## nsearch merge --forward=NG-13024_6_lib237855_5794_7_1.fastq.gz --reverse=NG-13024_6_lib237855_5794_7_2.fastq.gz --out=NG-13024_6.fastq

## ls *.fastq | grep -v lib | while read file; do nsearch filter --in $file --out filt_$file; done

## Read counts and other tabular data are provided by Python script filt_reads.py

library(tidyverse)
library(iNEXT)

mbcc <-
    read_csv("mol_bc_counts.csv") %>%
    separate(Sample_type, c("Sample", "Sample_replicate"), sep = "_") %>% 
    separate(Molecule_type, c("Stuffer", "Molecule_target", "Molecule_replicate"), sep = "_") %>% 
    filter(!(Molecule_target %in% c(9, 12)))

mol_counts <- 
    mbcc %>%
    count(Sample, Sample_replicate, Molecule_target, Molecule_replicate, name = "Count") %>% 
    mutate(Molecule_target = factor(Molecule_target, levels = as.character(c(0:10, 12:15))),
           Sample = factor(Sample, levels = as.character(c(1:11, 13:20))))

detection_limits <-
    mol_counts %>% 
    filter(Sample %in% c(3, 19, 20)) %>% 
    group_by(Molecule_target, Molecule_replicate) %>% 
    summarise(Mean = mean(Count),
              Sd = sd(Count)) %>% 
    mutate(Detlim = Mean + 3 * Sd)

##########################################
## False positive / negative estimation ##
##########################################

mol_counts %>% 
    filter(Sample %in% 3:8) %>% 
    ggplot(aes(x = Molecule_target, y = Count, color = Molecule_replicate)) +
    geom_boxplot() +
    facet_grid(Sample~.) +
    theme_bw() +
    theme(legend.position = "none")

mol_counts %>% 
    filter(Sample %in% 9:11) %>% 
    ggplot(aes(x = Molecule_target, y = Count, color = Molecule_replicate)) +
    geom_boxplot() +
    facet_grid(Sample~.) +
    theme_bw() +
    theme(legend.position = "none")

mol_counts %>% 
    filter(Sample %in% 13:20) %>% 
    ggplot(aes(x = Molecule_target, y = Count, color = Molecule_replicate)) +
    geom_boxplot() +
    facet_grid(Sample~.) +
    theme_bw() +
    theme(legend.position = "none")


concs <- 
    read_csv2("conc_gradient.csv",
              col_types = cols(
                  Sample = col_character(),
                  Molecule_target = col_character(),
                  Molecule_replicate = col_character()))

########################
## Sampling estimates ##
########################

barcode_estimates <- 
    mbcc %>% 
    filter(Sample %in% 9:11) %>% 
    group_by(Sample,
             Molecule_target,
             Molecule_replicate,
             Sample_replicate) %>% 
    nest %>% 
    mutate(
        Estimate = map(data,
                     ~ iNEXT(.$Count, q = 0, datatype = "abundance") %>%
                       .$AsyEst %>%
                       as_tibble(rownames = "Measure") %>%
                       filter(Measure == "Species Richness"))) %>%
    unnest(Estimate) %>%
    select(-data) %>% 
    left_join(
        read_csv2("conc_gradient.csv",
                  col_types = cols(
                      Sample = col_character(),
                      Molecule_target = col_character(),
                      Molecule_replicate = col_character())),
        by = c("Sample",
             "Molecule_target",
             "Molecule_replicate")) %>% 
    unite(Tar_Rep, Molecule_target, Molecule_replicate,
          sep = "_", remove = FALSE)


detection_limits %>% 
    unite(TR, Molecule_target, Molecule_replicate, sep = "_") %>% 
    select(-Sd) %>% 
    pivot_longer(cols = c("Mean", "Detlim"),
                 names_to = "Parameter",
                 values_to = "Value") %>% 
    ggplot(aes(x = TR, y = Value, color = Parameter)) +
    geom_point()


##Estimated
barcode_estimates %>% 
    filter(!(Tar_Rep %in% c("4_1", "3_1", "0_1", "6_2", "8_1"))) %>% 
    ggplot(aes(x = Molecule_concentration, y = Estimator, color = Tar_Rep)) +
    geom_point() +
    geom_smooth(method='lm', se=FALSE) +
    geom_hline(yintercept = range(37.16)) +
    geom_hline(yintercept = range(60.75), linetype="dashed") +
    geom_hline(yintercept = range(107.93), linetype="dotted") +
    scale_x_log10() +
    scale_y_log10() +
    theme_bw()

##Observed
barcode_estimates %>% 
    filter(!(Tar_Rep %in% c("4_1", "3_1", "0_1", "6_2", "8_1"))) %>% 
    ggplot(aes(x = Molecule_concentration, y = Observed, color = Tar_Rep)) +
    geom_point() +
    geom_smooth(method='lm', se=FALSE) +
    geom_hline(yintercept = range(37.16)) +
    geom_hline(yintercept = range(60.75), linetype="dashed") +
    geom_hline(yintercept = range(107.93), linetype="dotted") +
    scale_x_log10() +
    scale_y_log10() +
    theme_bw()


## linear_equations <- 
##     barcode_estimates %>% 
##     filter(!(Tar_Rep %in% c("4_1", "3_1", "0_1", "6_2", "8_1"))) %>% 
##     group_by(Tar_Rep) %>% 
##     select(Molecule_concentration, Estimator, Observed) %>% 
##     nest %>% 
##     mutate(Coefficient = map(data,
##                              ~ lm(Molecule_concentration ~ Observed, data = .) %>%
##                                .$coefficients)) %>% 
##     select(-data) %>% 
##     unnest(Coefficient) %>% 
##     mutate(Parameter = c("Intercept", "Slope")) %>% 
##     pivot_wider(
##         id_cols = "Tar_Rep",
##         names_from = "Parameter",
##         values_from = "Coefficient")


## linear_equations <- 
##     barcode_estimates %>% 
##     filter(!(Tar_Rep %in% c("4_1", "3_1", "0_1", "6_2", "8_1"))) %>% 
##     group_by(Tar_Rep) %>% 
##     select(Molecule_concentration, Estimator, Observed) %>% 
##     nest %>% 
##     mutate(
##         Estimated_Coefficients = map(data,
##                              ~ lm(Estimator ~ Molecule_concentration, data = .) %>%
##                                  .$coefficients),
##         Observed_Coefficients = map(data,
##                              ~ lm(Observed ~ Molecule_concentration, data = .) %>%
##                                  .$coefficients)) %>% 
##     select(-data) %>% 
##     unnest(c(Estimated_Coefficients, Observed_Coefficients)) %>% 
##     mutate(Parameter = c("Intercept", "Slope")) %>% 
##     pivot_wider(
##         id_cols = "Tar_Rep",
##         names_from = "Parameter",
##         values_from = c("Estimated_Coefficients", "Observed_Coefficients"))


## linear_equations <- 
##     barcode_estimates %>% 
##     filter(!(Tar_Rep %in% c("4_1", "3_1", "0_1", "6_2", "8_1"))) %>% 
##     group_by(Tar_Rep) %>% 
##     select(Molecule_concentration, Estimator, Observed) %>% 
##     nest %>% 
##     mutate(Coefficient = map(data,
##                              ~ lm(Estimator ~ Molecule_concentration, data = .) %>%
##                                .$coefficients)) %>% 
##     select(-data) %>% 
##     unnest(Coefficient) %>% 
##     mutate(Parameter = c("Intercept", "Slope")) %>% 
##     pivot_wider(
##         id_cols = "Tar_Rep",
##         names_from = "Parameter",
##         values_from = "Coefficient")


m - slope
y0 - intercept

y = mx + y0

x -> spike_in
y -> observed

x = (y - y0) / m
x = (y - intercept) / slope


## concs_tbl <- 
##     read_csv2("conc_gradient.csv",
##               col_types = cols(
##                   Sample = col_character(),
##                   Molecule_target = col_character(),
##                   Molecule_replicate = col_character())) %>% 
##     unite(Tar_Rep, Molecule_target, Molecule_replicate,
##           sep = "_")


linear_equations <- 
    barcode_estimates %>% 
    filter(!(Tar_Rep %in% c("4_1", "3_1", "0_1", "6_2", "8_1"))) %>% 
    group_by(Tar_Rep) %>% 
    select(Molecule_concentration, Estimator, Observed) %>% 
    nest %>% 
    mutate(
        Estimated_Coefficients = map(data,
                             ~ lm(Estimator ~ Molecule_concentration, data = .) %>%
                                 .$coefficients),
        Observed_Coefficients = map(data,
                             ~ lm(Observed ~ Molecule_concentration, data = .) %>%
                                 .$coefficients)) %>% 
    select(-data) %>% 
    unnest(c(Estimated_Coefficients, Observed_Coefficients)) %>% 
    mutate(Parameter = c("Intercept", "Slope")) %>% 
    pivot_wider(
        id_cols = "Tar_Rep",
        names_from = "Parameter",
        values_from = c("Estimated_Coefficients", "Observed_Coefficients"))



background_estimates <-
    mbcc %>% 
    filter(Sample %in% c(3, 19, 20)) %>% 
    group_by(Sample,
             Molecule_target,
             Molecule_replicate,
             Sample_replicate) %>% 
    nest %>% 
    mutate(
        Estimate = map(data,
                       ~ iNEXT(.$Count, q = 0, datatype = "abundance") %>%
                           .$AsyEst %>%
                         as_tibble(rownames = "Measure") %>%
                         filter(Measure == "Species Richness"))) %>%
    unnest(Estimate) %>%
    select(-data) %>% 
    ungroup %>% 
    group_by(Molecule_target, Molecule_replicate) %>% 
    summarise(Obs_Mean = mean(Observed),
              Obs_Sd = sd(Observed),
              Est_Mean = mean(Estimator),
              Est_Sd = sd(Estimator)) %>% 
    mutate(Obs_Detlim = Obs_Mean + 3 * Obs_Sd,
           Est_Detlim = Est_Mean + 3 * Est_Sd)


lods <- 
    background_estimates %>% 
    unite(Tar_Rep, Molecule_target, Molecule_replicate,
          sep = "_") %>% 
    left_join(linear_equations) %>% 
    mutate(Estimated_LoD = (Est_Detlim - Estimated_Coefficients_Intercept) / Estimated_Coefficients_Slope,
           Observed_LoD = (Obs_Detlim - Observed_Coefficients_Intercept) / Observed_Coefficients_Slope) %>% 
    filter(complete.cases(.))


pdf("lods_obs_long.pdf", useDingbats = FALSE)
ggplot(data = lods) +
    scale_x_continuous(limits=c(0, 1.5e8)) + 
    scale_y_continuous(limits=c(0, 2500)) + 
    geom_abline(aes(slope = Observed_Coefficients_Slope,
                                 intercept = Observed_Coefficients_Intercept,
                    color = Tar_Rep)) +
    geom_point(aes(x = Observed_LoD, y = Obs_Detlim, color = Tar_Rep)) +
    theme_bw() +
    guides(color=FALSE)
dev.off()


pdf("lods_obs_short.pdf", useDingbats = FALSE)
ggplot(data = lods) +
    scale_x_continuous(limits=c(0, 5e7)) + 
    scale_y_continuous(limits=c(0, 1000)) + 
    geom_abline(aes(slope = Observed_Coefficients_Slope,
                                 intercept = Observed_Coefficients_Intercept,
                    color = Tar_Rep)) +
    geom_point(aes(x = Observed_LoD, y = Obs_Detlim, color = Tar_Rep)) +
    theme_bw() +
    guides(color=FALSE)
dev.off()

    
    
## pdf("lods.pdf", useDingbats = FALSE)
## ggplot(data = lods) +
##     scale_x_continuous(limits=c(0, 2e8)) + 
##     scale_y_continuous(limits=c(0, 6500)) + 
##     geom_abline(aes(slope = Estimated_Coefficients_Slope,
##                                  intercept = Estimated_Coefficients_Intercept,
##                     color = Tar_Rep)) +
##     geom_point(aes(x = Estimated_LoD, y = Est_Detlim, color = Tar_Rep)) +
##     theme_bw() +
##     guides(color=FALSE)
## dev.off()


## pdf("lods_annot.pdf", useDingbats = FALSE)
## ggplot(data = lods) +
##     scale_x_continuous(limits=c(-(1e8), 1e8)) + 
##     scale_y_continuous(limits=c(0, 6500)) + 
##     geom_abline(aes(slope = Estimated_Coefficients_Slope,
##                                  intercept = Estimated_Coefficients_Intercept,
##                     color = Tar_Rep)) +
##     geom_point(aes(x = Estimated_LoD, y = Est_Detlim, color = Tar_Rep)) +
##     theme_bw()
## dev.off()

xxx

## lods <- 
##     detection_limits %>% 
##     unite(Tar_Rep, Molecule_target, Molecule_replicate,
##           sep = "_") %>% 
##     left_join(linear_equations) %>% 
##     mutate(Estimated_LoD = (Detlim - Estimated_Coefficients_Intercept) / Estimated_Coefficients_Slope,
##            Observed_LoD = (Detlim - Observed_Coefficients_Intercept) / Observed_Coefficients_Slope) %>% 
##     arrange(desc(Observed_LoD)) %>%
##     select(Tar_Rep, Detlim, Estimated_LoD, Observed_LoD) %>%
##     filter(complete.cases(.))





barcode_estimates %>% 
    filter(!(Tar_Rep %in% c("4_1", "3_1", "0_1", "6_2", "8_1"))) %>% 
    ggplot(aes(x = Molecule_concentration, y = Estimator, color = Tar_Rep)) +
    geom_smooth(method='lm', se=FALSE) +
    geom_point(data = lods, aes(x = Observed_LoD, y = Est_Detlim, color = Tar_Rep)) + 
    scale_x_log10() +
    scale_y_log10() +
    theme_bw()

geom_abline(data=df,aes(slope=slope,intercept=intercept,color=factor(wf)))


XXX

#+END_SRC


* 3) Session info

** Python version 3.6.7 | packaged by conda-forge | (default, Feb 25 2019, 20:30:30)

- Pandas version 0.24.1

** R version 3.5.1 (2018-07-02)
- Platform: x86_64-apple-darwin13.4.0 (64-bit)
- Running under: macOS  10.14.3

** Matrix products: default
- BLAS/LAPACK: /Users/mavatam/miniconda3/lib/R/lib/libRblas.dylib

** locale:

[1] C/UTF-8/C/C/C/C

** attached base packages:

[1] stats     graphics  grDevices utils     datasets  methods   base

** other attached packages:

 [1] forcats_0.4.0     stringr_1.4.0     dplyr_0.8.0.1     purrr_0.3.1

 [5] readr_1.3.1       tidyr_0.8.3       tibble_2.0.1      ggplot2_3.1.0

 [9] tidyverse_1.2.1   plyr_1.8.4        data.table_1.12.0 iNEXT_2.0.19

** loaded via a namespace (and not attached):

 [1] Rcpp_1.0.0       cellranger_1.1.0 pillar_1.3.1     compiler_3.5.1

 [5] tools_3.5.1      jsonlite_1.6     lubridate_1.7.4  gtable_0.2.0

 [9] nlme_3.1-137     lattice_0.20-38  pkgconfig_2.0.2  rlang_0.3.1

[13] cli_1.0.1        rstudioapi_0.9.0 haven_2.1.0      withr_2.1.2

[17] xml2_1.2.0       httr_1.4.0       generics_0.0.2   hms_0.4.2

[21] grid_3.5.1       tidyselect_0.2.5 glue_1.3.0       R6_2.4.0

[25] readxl_1.3.0     reshape2_1.4.3   modelr_0.1.4     magrittr_1.5

[29] scales_1.0.0     backports_1.1.3  rvest_0.3.2      assertthat_0.2.0

[33] colorspace_1.4-0 stringi_1.3.1    lazyeval_0.2.1   munsell_0.5.0

[37] broom_0.5.1      crayon_1.3.4

