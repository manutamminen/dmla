* Code and data for the Digital Multiplex Ligation Assay (dMLA) method validation

** Data files

 | File                          | Content                                           |
 |-------------------------------+---------------------------------------------------|
 | Allele-dna.fa                 | Reference sequences                               |
 | Allele.tab                    | Sequence annotation                               |
 | all_probes.xlsx               | Probe, template and primer sequences for Figure 2 |
 | probes.xlsx                   | Probe, template and primer sequences for Figure 3 |
 | libraries.xlsx                | Annotation for Figures 3 and S01                  |
 | Table_Gates_Probetargets.xlsx | Annotation for Figures 3 and S01                  |
 | sample_conversion.xlsx        | Annotation for Figures 3 and S01                  |
 | cluster_confirmation.xlsx     | Annotation for Figures 3 and S01                  |



* Figure 2

** Prepare the 40-mer probe candidates
   
|---------------+-------------------|
| Inputs        | Outputs           |
|---------------+-------------------|
| Allele-dna.fa | probe_cands.fasta |
|---------------+-------------------|

R

 #+BEGIN_SRC R
 library(tidyverse)
 library(readxl)
 library(Biostrings)
 library(igraph)
 library(DECIPHER)
 library(stringi)
 library(glue)
 library(readxl)
 library(stringr)

 sliding_window <- function(sequence, win_size=20)
 {
     win_size <- win_size - 1
     split_sequence <- strsplit(sequence, split="")[[1]]
     num_chunks <- length(split_sequence) - win_size
     acc <- vector(mode = "character",
                   length = num_chunks)
     for (i in 1:num_chunks)
     {
         sub_seq <- paste(split_sequence[i : (i + win_size)],
                          collapse = "")
         acc[i] <- sub_seq
     }
     acc
 }

 deg_list <-
     list(
         'A' = 'A',
         'T' = 'T',
         'G' = 'G',
         'C' = 'C',
         '-' = '-',
         'W' = c('A', 'T'),
         'S' = c('C', 'G'),
         'M' = c('A', 'C'),
         'K' = c('G', 'T'),
         'R' = c('A', 'G'),
         'Y' = c('C', 'T'),
         'B' = c('C', 'G', 'T'),
         'D' = c('A', 'G', 'T'),
         'H' = c('A', 'C', 'G'),
         'V' = c('A', 'C', 'T'),
         'N' = c('A', 'C', 'G', 'T'))

 expand_seq <- function(seq)
 {
     seq_lst <-
         strsplit(seq, "") %>%
         unlist %>%
         map(~deg_list[[.x]]) %>%
         purrr::reduce(~as.vector(outer(.x, .y, paste, sep="")))
     if (identical(seq_lst, character(0)))
     {
         stop("Not a DNA sequence!")
     } else {
         seq_lst
     }
 }

 fasta_to_df <- function(filename)
 {
     fasta <- readDNAStringSet(filename)
     seqs <- as.character(fasta)
     names(seqs) <- NA
     tibble(Name = names(fasta),
            Sequence = seqs)
 }
 
 primer_candidates <-
     fasta_to_df("Allele-dna.fa") %>% 
     mutate(Exp = map(Sequence,
                      sliding_window(as.character,
                                     win_size = 40))) %>%
     select(-Sequence) %>%
     unnest(Exp) %>%
     group_by(Name) %>%
     mutate(Ix = row_number()) %>%
     separate(Name, into=c("Prot_id"), sep=" ") %>%
     unite(Fasta_id, Prot_id, Ix, sep="_")

 primer_candidates %>% 
     mutate(Out = glue(">{Fasta_id}\n{Exp}\n")) %>% 
     pull(Out) %>%
     write("probe_cands.fasta")
 #+END_SRC


** Prepare the BLAST search table

|-------------------+----------------|
| Inputs            | Outputs        |
|-------------------+----------------|
| probe_cands.fasta | probe_hits.csv |
| Allele-dna.fa     |                |
|-------------------+----------------|

Bash

 #+BEGIN_SRC sh 
 nsearch search --query=probe_cands.fasta --db=Allele-dna.fa --out=probe_hits.csv --min-identity=0.8 --strand=both --max-hits=1558
 #+END_SRC


** Then parse the resulting output file "probe_hits.csv" using a memory-efficient Python script

|----------------+------------------|
| Inputs         | Outputs          |
|----------------+------------------|
| probe_hits.csv | probe_counts.csv |
|----------------+------------------|

Python

 #+BEGIN_SRC python
 import sys
 from collections import defaultdict

 acc = defaultdict(int)
 with open('probe_hits.csv') as fh:
     next(fh)
     for ix, ln in enumerate(fh):
         broken = ln.split(",")
         fst = broken[0].replace("WP_", "WP").split("_")[0]
         fst = fst.replace("WP", "WP_")
         snd = broken[1].replace("WP_", "WP").split(" ")[0]
         snd = snd.replace("WP", "WP_")
         qlength = int(broken[3]) - int(broken[2])
         tlength = int(broken[5]) - int(broken[4])
         to_acc = ",".join(sorted([fst, snd]))
         if ((qlength == tlength) and (qlength == 39) and (fst != snd)):
             acc[to_acc] += 1
         if (ix % 100000 == 0):
             print(ix)

 with open('probe_counts.csv', 'w') as fh:
     for key, val in acc.items():
         fh.write(key + "\n")
 #+END_SRC


** Prepare also the gdf such that our primer designs are also shown in the network

*** Start by expanding our probe designs (all_probes.xlsx) into non-degenerate versions

|-----------------+----------------|
| Inputs          | Outputs        |
|-----------------+----------------|
| all_probes.xlsx | exp_probes.csv |
|-----------------+----------------|

R

 #+BEGIN_SRC R :session
 read_excel("all_probes.xlsx", sheet = "probes") %>% 
     mutate(Exp = map(Target, expand_seq)) %>%
     unnest %>% 
     write_csv(exp_probes, "exp_probes.csv")
 #+END_SRC


*** Then filter out their target ranges using a memory-efficient Python script

|----------------+-------------------------|
| Inputs         | Outputs                 |
|----------------+-------------------------|
| exp_probes.csv | selected_probe_hits.csv |
| probe_hits.csv |                         |
|----------------+-------------------------|

Python

 #+BEGIN_SRC python
 seq_acc = set()
 with open("exp_probes.csv") as ep:
     next(ep)
     for ix, line in enumerate(ep):
         seq = line.split(",")[4].strip()
         seq_acc.add(seq)

 probe_acc = []
 with open("probe_hits.csv") as ph:
     next(ph)
     for ix, line in enumerate(ph):
         seq = line.split(",")[6]
         if seq in seq_set:
             probe_acc.append(line)
         if (ix % 1000 == 0):
             print(ix)
        
 with open("selected_probe_hits.csv", "w") as out:
     for line in probe_acc:
         out.write(line)
 #+END_SRC


*** Process the resulting selected probe hits file "selected_probe_hits.csv" into gdf annotation

|-------------------------+--------------|
| Inputs                  | Outputs      |
|-------------------------+--------------|
| selected_probe_hits.csv | clusters.gdf |
| probe_counts.csv        |              |
| Allele.tab              |              |
|-------------------------+--------------|

R

 #+BEGIN_SRC R :session
 selected_hits <-
     read_csv("selected_probe_hits.csv", col_names=FALSE)

 exp_probes <-
     read_csv("exp_probes.csv")

 probe_coverage <-
     left_join(exp_probes, selected_hits, by=c("Exp" = "X7")) %>%
     select(Name, X1) %>%
     filter(complete.cases(.)) %>%
     unique %>%
     mutate(X1 = str_replace(X1, "WP_", "WP")) %>%
     separate(X1, c("Seq"), "_") %>%
     mutate(Seq = str_replace(Seq, "WP", "WP_")) %>%
     unique %>%
     group_by(Seq) %>%
     summarise(Probes = paste(sort(Name), collapse=";"))

 con <-
     read_csv("probe_counts.csv", col_names=FALSE) %>%
     unite(Netw, X1, X2, sep=",") %>%
     pull(Netw)

 annotation <-
     read.delim("Allele.tab", sep="\t") %>%
     separate(allele_name, into=c("type"), sep="-", remove=FALSE) %>%
     mutate(size = stop - start) %>%
     select(protein_accession, type, size) %>%
     left_join(probe_coverage, by=c("protein_accession" = "Seq"))

 gdf_annotation <- 
     annotation %>%
     with(paste(protein_accession, type, size, Probes, sep=","))

 gdf <-
     c("nodedef>name VARCHAR,type VARCHAR,size DOUBLE,probe VARCHAR",
       gdf_annotation,
       "edgedef>node1 VARCHAR,node2 VARCHAR",
       con)
         
 write(gdf, "clusters.gdf")
 #+END_SRC

 
* Figure 3

** Sequencing stats

*** Raw sequences

Bash

#+BEGIN_SRC sh
ls *.gz | while read file; do echo $file; gzcat $file | wc -l | awk '{print $1 / 4}'; done
#+END_SRC

*** After read merging

Bash

#+BEGIN_SRC sh
ls *.fastq | while read file; do echo $file; wc -l $file | awk '{print $1 / 4}'; done
#+END_SRC

*** After quality filtering

Bash

#+BEGIN_SRC sh
ls lib*.fasta | while read file; do echo $file; grep -c ">" $file; done
#+END_SRC

*** Put together

|---------+---------+---------------+----------------------|
| Library |     Raw | After merging | After quality filter |
|---------+---------+---------------+----------------------|
| lib10   | 6802543 |       6731594 |              6696375 |
| lib11_1 | 3873555 |       3677900 |              3648867 |
| lib11_2 | 5111334 |       4837779 |              4802951 |
|---------+---------+---------------+----------------------|



** Join the paired ends and quality filter using nsearch

|-----------------------------------------+---------------|
| Inputs                                  | Outputs       |
|-----------------------------------------+---------------|
| NG-17872_10_lib297291_6185_1_1.fastq.gz | lib10.fasta   |
| NG-17872_11_lib297292_6178_3_1.fastq.gz | lib11_1.fasta |
| NG-17872_11_lib297292_6189_3_1.fastq.gz | lib11_2.fasta |
| NG-17872_10_lib297291_6185_1_1.fastq.gz |               |
| NG-17872_11_lib297292_6178_3_1.fastq.gz |               |
| NG-17872_11_lib297292_6189_3_1.fastq.gz |               |
|-----------------------------------------+---------------|

Bash

 #+BEGIN_SRC sh
 nsearch merge --forward NG-17872_10_lib297291_6185_1_1.fastq.gz --reverse NG-17872_10_lib297291_6185_1_2.fastq.gz --out lib10.fastq
 nsearch merge --forward NG-17872_11_lib297292_6178_3_1.fastq.gz --reverse NG-17872_11_lib297292_6178_3_2.fastq.gz --out lib11_1.fastq
 nsearch merge --forward NG-17872_11_lib297292_6189_3_1.fastq.gz --reverse NG-17872_11_lib297292_6189_3_2.fastq.gz --out lib11_2.fastq

 nsearch filter --in lib10.fastq --out lib10.fasta
 nsearch filter --in lib11_1.fastq --out lib11_1.fasta
 nsearch filter --in lib11_2.fastq --out lib11_2.fasta
 #+END_SRC


** Then process the merged, quality-filtered sequences into count tables on Python

|---------------+-------------|
| Inputs        | Outputs     |
|---------------+-------------|
| lib10.fasta   | lib10.csv   |
| lib11_1.fasta | lib11_1.csv |
| lib11_2.fasta | lib11_2.csv |
| probes.xlsx   |             |
|---------------+-------------|

Python

 #+BEGIN_SRC python
 import os
 import epride as ep
 import pandas as pd
 from collections import defaultdict

 ## Import the data

 probes = pd.ExcelFile("probes.xlsx").parse('probes')
 pcr_bcs = pd.ExcelFile("probes.xlsx").parse('pcr_barcodes').drop('Sequence', axis=1)
 other_sequences = pd.ExcelFile("probes.xlsx") \
                     .parse('other_primers_and_sequences') \
                     .set_index('Sequence_name')
 left_side = other_sequences.loc['for_primer_5', 'Sequence']
 middle = other_sequences.loc['left_probe_5', 'Sequence']
 right_side = other_sequences.loc['rev_primer_rc', 'Sequence'][:20]


 ## Create the template, sample id and bc number dictionaries

 template_dictionary = {}
 for _, row in probes.iterrows():
     for seq in ep.expand_primers(row['Target']):
         template_dictionary[seq] = row['Short_name']

 sample_id_dict = {bc: bc_id for _, (_, bc_id, bc) in pcr_bcs.iterrows()}
 sample_ix_dict = {bc: ix for _, (ix, _, bc) in pcr_bcs.iterrows()}


 ## Define the sequence parser

 def seq_parser(fasta_file):
     for seq_id, seq in ep.read_fasta(fasta_file):
         if (len(seq) > 133 or len(seq) < 140) and \
         seq.count(left_side) == 1 and \
         seq.count(middle) == 1 and \
         seq.count(right_side) == 1:
             cluster_id = ''
             try:
                 fst_half, long_mid_part = seq.split(middle)
                 _, bc = fst_half.split(left_side)
                 mid_part, _ = long_mid_part.split(right_side)
                 mol_id = mid_part[-10:]
                 cluster_id = mid_part[8:-10]
                 if bc in sample_id_dict:
                     sample_id = sample_id_dict[bc]
                     sample_ix = sample_ix_dict[bc]
             except ValueError:
                 pass
             if cluster_id in template_dictionary:
                 cluster = template_dictionary[cluster_id]
                 yield [sample_ix, sample_id, cluster, mol_id]

 ## And parse the sequences into pandas DataFrames

 lib10 = pd.DataFrame(seq_parser("lib10.fasta"),
                      columns=['Sample_ix',
                               'Sample_id',
                               'Cluster',
                               'Molecule_id'])

 lib11_1 = pd.DataFrame(seq_parser("lib11_1.fasta"),
                        columns=['Sample_ix',
                                 'Sample_id',
                                 'Cluster',
                                 'Molecule_id'])

 lib11_2 = pd.DataFrame(seq_parser("lib11_2.fasta"),
                        columns=['Sample_ix',
                                 'Sample_id',
                                 'Cluster',
                                 'Molecule_id'])

 ## And write out as csvs

 lib10.to_csv("lib10.csv", index=False)
 lib11_1.to_csv("lib11_1.csv", index=False)
 lib11_2.to_csv("lib11_2.csv", index=False)
 #+END_SRC

 
** Expand the library file (which lists the gene families present in the bacterial genomic DNA samples)

|----------------+-------------------------|
| Inputs         | Outputs                 |
|----------------+-------------------------|
| libraries.xlsx | expanded_libraries.xlsx |
|----------------+-------------------------|

Python

#+BEGIN_SRC python :session
import os
import epride as ep
import pandas as pd
from collections import defaultdict

## Import the data

libraries = pd.read_excel("libraries.xlsx")

## Expand the table based in the numeric Cluster column

acc = []
for _, row in libraries.iterrows():
    cluster = row['Cluster']
    if isinstance(cluster, int):
        row1 = row.copy().to_dict()
        row2 = row.copy().to_dict()
        row1['Cluster'] = str(cluster) + "_1"
        row2['Cluster'] = str(cluster) + "_2"
        acc.append(row1)
        acc.append(row2)
    elif "," in cluster:
        exp_cluster = cluster.split(",")
        for cluster_instance in exp_cluster:
            try:
                cluster_instance = int(cluster_instance)
                row1 = row.copy().to_dict()
                row2 = row.copy().to_dict()
                row1['Cluster'] = str(cluster_instance) + "_1"
                row2['Cluster'] = str(cluster_instance) + "_2"
                acc.append(row1)
                acc.append(row2)
            except ValueError:
                pass

exp_libraries = pd.DataFrame(acc)[['Number',
                                   'Sample_ID',
                                   'Genes',
                                   'Cluster',
                                   'Probes_in_MM_included',
                                   'Sample_ix',
                                   'Tube']]

exp_libraries.to_excel("expanded_libraries.xlsx", index=False)
#+END_SRC


** Prepare visualizations of the lib10 and lib11 count tables

|-------------------------------+-------------|
| Inputs                        | Outputs     |
|-------------------------------+-------------|
| expanded_libraries.xlsx       | Fig_3.pdf   |
| lib10.csv                     | Fig_S01.pdf |
| lib11_1.csv                   |             |
| lib11_2.csv                   |             |
| Table_Gates_ProbeTargets.xlsx |             |
| cluster_confirmation.xlsx     |             |
| sample_conversion.xlsx        |             |
|-------------------------------+-------------|

R

 #+BEGIN_SRC R :session
 library(tidyverse)
 library(readxl)

 lib10_counts <-
     read_csv("lib10.csv") %>%
     unique %>%
     group_by(Sample_ix, Cluster) %>%
     summarise(n=n()) %>%
     spread(key=Cluster, value=n, fill=0) %>%
     ungroup %>%
     mutate(Sample_ix = as.factor(Sample_ix)) %>%
     gather(Cluster, Count, -Sample_ix) %>%
     mutate(Tube = 10) %>%
     select(Tube, Sample_ix, Cluster, Count)

 lib11_counts <-
     rbind(
         read_csv("lib11_1.csv"),
         read_csv("lib11_2.csv")) %>%
     unique %>%
     group_by(Sample_ix, Cluster) %>%
     summarise(n=n()) %>%
     spread(key=Cluster, value=n, fill=0) %>%
     ungroup %>%
     mutate(Sample_ix = as.factor(Sample_ix)) %>%
     gather(Cluster, Count, -Sample_ix) %>%
     mutate(Tube = 11) %>%
     select(Tube, Sample_ix, Cluster, Count)

 lib_counts <-
     rbind(lib10_counts, lib11_counts) %>%
     spread(Cluster, Count, fill=0) %>%
     gather(Cluster, Count, -Tube, -Sample_ix)  %>%
     spread(Sample_ix, Count, fill=0) %>%
     gather(Sample_ix, Count, -Tube, -Cluster)

 design <-
     read_excel("expanded_libraries.xlsx") %>%
     mutate(Entry = 1) %>%
     select(Tube, Cluster, Tube, Sample_ix, Entry) %>%
     unique %>%
     spread(Cluster, Entry, fill=0) %>%
     mutate(Sample_ix = as.factor(Sample_ix)) %>%
     gather(Cluster, Count, -Sample_ix, -Tube) %>%
     mutate(Mask = Count > 0) %>%
     select(-Count)

 full_lib <-
     left_join(lib_counts,
               design,
               by=c("Tube", "Sample_ix", "Cluster")) %>%
     mutate_if(is.logical, replace_na, FALSE) %>%
     mutate(Cluster = as.factor(Cluster))

 cluster_summary <-
     filter(full_lib, Count > 500) %>%
     separate(Cluster, into=c("Cluster_no", "Cluster_repl"), sep="_") %>%
     select(-Cluster_repl, -Mask, -Count) %>%
     group_by(Tube, Sample_ix) %>%
     summarise(Clusters = paste(unique(Cluster_no), collapse=","))
 write_delim(cluster_summary, "cluster_summary.csv", delim=";")

 t10_dl <- 
     filter(full_lib, Tube == 10,
            Sample_ix %in% c(511, 512, 513)) %>%
     group_by(Cluster) %>% 
     summarise(mean_cnt = mean(Count, na.rm = TRUE),
               sd_cnt = sd(Count, na.rm = TRUE),
               ld = mean_cnt + 3*sd_cnt)

 clust_conv <- 
     read_xlsx("Table_Gates_ProbeTargets.xlsx") %>%
     select(Cluster, `Enzyme family`) %>%
     filter(complete.cases(.)) %>%
     rename(Clust = Cluster,
            Family = `Enzyme family`)

 conf_mask <-
     read_xlsx("cluster_confirmation.xlsx") %>% 
     gather(Cf, Conf, -Sample_ix, -False1, -False2, -False3) %>%
     select(-Cf) %>%
     gather(Fl, False_pos, -Sample_ix, -Conf) %>%
     select(-Fl) %>%
     filter(!(is.na(Conf) & is.na(False_pos))) %>%
     unique %>%
     mutate(Category = case_when(
                !is.na(Conf) ~ 3,
                !is.na(False_pos) ~ 4)) %>%
     gather(Type, Clust, -Sample_ix, -Category) %>%
     filter(complete.cases(.)) %>%
     select(-Type)

 lib1 <- 
     full_lib %>% 
     mutate(Count = ifelse(Mask, Count, -Count),
            Tube = as.factor(Tube)) %>% 
     filter(Tube == 10) %>% 
     separate(Cluster, c("Clust", "Repl"), sep="_") %>% 
     mutate(Sample_ix = as.numeric(Sample_ix),
            Clust = as.numeric(Clust),
            Count = abs(Count)) %>%
     left_join(read_xlsx("sample_conversion.xlsx")) %>%
     left_join(clust_conv) %>% 
     filter(Repl != 3,
            Family != "KPC") %>% 
     select(Family, Repl, Sample_ID, Count)

lib2 <-
    signal_tbl %>%
    select(Family, Repl, Sample_ID, Sign)
 
 signal_tbl <- 
     full_lib %>% 
     filter(Tube == 10) %>% 
     filter(!(Sample_ix %in% c(511, 512, 513))) %>%
     left_join(t10_dl, by="Cluster") %>%
     ungroup %>% 
     mutate(Signal = ifelse(Count > ld, 1, 0),
            Sample_ix = as.numeric(Sample_ix)) %>% 
     separate(Cluster, c("Clust", "Repl"), sep="_") %>%
     group_by(Sample_ix, Clust) %>%
     mutate(Signal = ifelse(sum(Signal) == 2, 1, 0),
            Signal = ifelse(Mask, Signal, -Signal)) %>%
     ungroup %>%
     mutate(Clust = as.numeric(Clust)) %>%
     full_join(conf_mask) %>%
     left_join(read_xlsx("sample_conversion.xlsx")) %>%
     left_join(clust_conv) %>%
     mutate(Sign = case_when(
                Signal == -1 ~ 2,
                is.na(Category) ~ Signal,
                !is.na(Category) ~ Category),
            Sign = ifelse(Signal == 0, 0, Sign),
            Sign = as.factor(Sign))

 left_join(lib1, signal_tbl) %>% 
     unite(Fam_rep, Family, Repl, sep=" replicate") %>% 
     mutate(Signal = abs(Signal)) %>% 
     ggplot(aes(x=Sample_ID, y=Count, fill = Signal)) +
     geom_bar(stat = "identity") +
     geom_hline(aes(yintercept = ld), alpha = 0.1) + 
     facet_grid(Fam_rep ~ ., scales = "free") +
     theme(strip.text.y = element_text(angle = 0, size = 6),
           axis.text.x = element_text(angle = 45, hjust = 1, size = 3),
           axis.text.y = element_text(size = 3),
           panel.grid.major = element_blank(),
           panel.grid.minor = element_blank(),
           panel.border = element_blank(),
           panel.background = element_blank())
 ggsave("Fig_s01.pdf", last_plot())

 left_join(lib1, lib2) %>% 
     unite(Fam_rep, Family, Repl, sep=" replicate") %>% 
     ggplot(aes(x=Fam_rep, y=Sample_ID)) +
     geom_tile(aes(fill=Count, color=Sign), size=1) +
     scale_fill_gradient2(low = "blue", high = "red", mid="white") +
     theme(axis.text.x = element_text(angle=45, hjust=1, size=7),
           axis.text.y = element_text(size=5))
 ggsave("Fig_3.pdf", last_plot())
 #+END_SRC



* Session info

** Python version 3.6.7 | packaged by conda-forge | (default, Feb 25 2019, 20:30:30)

- Pandas version 0.24.1

** R version 3.5.1 (2018-07-02)
- Platform: x86_64-apple-darwin13.4.0 (64-bit)
- Running under: macOS  10.14.3

** Matrix products: default
- BLAS/LAPACK: /Users/mavatam/miniconda3/lib/R/lib/libRblas.dylib

** locale:

[1] C/UTF-8/C/C/C/C

** attached base packages:

[1] stats     graphics  grDevices utils     datasets  methods   base

** other attached packages:

 [1] forcats_0.4.0     stringr_1.4.0     dplyr_0.8.0.1     purrr_0.3.1

 [5] readr_1.3.1       tidyr_0.8.3       tibble_2.0.1      ggplot2_3.1.0

 [9] tidyverse_1.2.1   plyr_1.8.4        data.table_1.12.0 iNEXT_2.0.19

** loaded via a namespace (and not attached):

 [1] Rcpp_1.0.0       cellranger_1.1.0 pillar_1.3.1     compiler_3.5.1

 [5] tools_3.5.1      jsonlite_1.6     lubridate_1.7.4  gtable_0.2.0

 [9] nlme_3.1-137     lattice_0.20-38  pkgconfig_2.0.2  rlang_0.3.1

[13] cli_1.0.1        rstudioapi_0.9.0 haven_2.1.0      withr_2.1.2

[17] xml2_1.2.0       httr_1.4.0       generics_0.0.2   hms_0.4.2

[21] grid_3.5.1       tidyselect_0.2.5 glue_1.3.0       R6_2.4.0

[25] readxl_1.3.0     reshape2_1.4.3   modelr_0.1.4     magrittr_1.5

[29] scales_1.0.0     backports_1.1.3  rvest_0.3.2      assertthat_0.2.0

[33] colorspace_1.4-0 stringi_1.3.1    lazyeval_0.2.1   munsell_0.5.0

[37] broom_0.5.1      crayon_1.3.4



